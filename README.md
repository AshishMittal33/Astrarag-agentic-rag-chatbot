# ğŸ›°ï¸ AstraRAG â€” Agentic RAG Chatbot (Production-Grade)

AstraRAG is a production-grade Agentic RAG system, not just a chatbot.
It is a cloud-deployed AI agent capable of executing complex, multi-step tasks using tools, retrieval, and reasoning â€” while explicitly showing tool usage to reduce hallucinations and increase trust.

The system is designed with scalability, transparency, and real-world deployment in mind.

# âœ¨ Key Features

ğŸ§  Agentic RAG (not a simple chatbot)

ğŸ› ï¸ Tool-aware AI agent (visible tool usage)

ğŸ” Retrieval-Augmented Generation (RAG)

ğŸš« Reduced hallucination via grounded answers

âš¡ FastAPI backend for scalability

ğŸ¨ Streamlit UI for interaction

ğŸ³ Fully Dockerized application

â˜ï¸ Deployed on AWS EC2 (production setup)

# ğŸ›  Tech Stack

1. Agent Framework: CrewAI

2. Backend API: FastAPI

3. UI: Streamlit

4. LLM Provider: Groq

5. RAG Framework: LlamaIndex

6. Vector Store: ChromaDB

7. Containerization: Docker

8. Cloud Hosting: AWS EC2

9. Language: Python

# ğŸ“¦ Dependencies

python-dotenv==1.1.1

pydantic==2.11.9

pydantic-settings==2.10.1

crewai==0.186.1

fastapi==0.116.1

uvicorn==0.35.0

streamlit==1.49.1

chromadb==1.0.21

llama-index==0.14.0

llama-index-embeddings-huggingface==0.6.1

llama-index-llms-groq==0.4.1

llama-index-vector-stores-chroma==0.5.3

numpy==1.26.4

# ğŸ§  How AstraRAG Works

- User submits a complex query

- CrewAI agent plans the task

- Relevant context retrieved using RAG (ChromaDB + LlamaIndex)

- Agent selects and uses tools explicitly

- Tool usage is shown for transparency

- Final response is generated by Groq LLM

- Answer is grounded and less prone to hallucination

# ğŸ³ Docker & Production Deployment

- Entire application is Dockerized

- Backend (FastAPI) and UI (Streamlit) run as services

- Deployed on AWS EC2

- Environment variables managed securely

- Production-ready setup using uvicorn

- This mirrors real-world AI SaaS deployment patterns.

#ğŸš€ Run Locally

1. pip install -r requirements.txt


2. Create a .env file:

  GROQ_API_KEY=your_api_key_here


3. Run backend:

  uvicorn app.main:app --host 0.0.0.0 --port 8000


4. Run UI:

  streamlit run ui.py

# â˜ï¸ Deployment

- Dockerized application deployed on AWS EC2

- Backend served via FastAPI + Uvicorn

- UI exposed via Streamlit

- Suitable for scaling and future service separation

# ğŸ“¹ Demo Video:

https://www.youtube.com/watch?v=BGDccaXHPXc

# ğŸ¯ Why This Project Matters

This project demonstrates:

1. Agentic AI system design

2. Production-ready RAG architecture

3. Hallucination-aware AI responses

4. Tool-based reasoning and transparency

5. Docker + cloud deployment skills

This is not a demo chatbot â€” it is a foundation for real AI products.

# ğŸ‘¨â€ğŸ’» Author

Ashish Mittal

Generative AI Developer

# â­ If you find this project useful, give it a star!


